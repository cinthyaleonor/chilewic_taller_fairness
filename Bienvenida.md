# ğŸ¤– Taller de Fairness en Inteligencia Artificial âš–ï¸ğŸ‘©â€ğŸ’»

## ğŸ¯ Objetivo del Taller
- ğŸ“š Analizar implicaciones Ã©ticas y de equidad

- ğŸ”„ Desarrollar soluciones creativas

- ğŸ” Identificar oportunidades y desafÃ­os en IA

## ğŸ•µï¸ Caso de Estudio: ContrataciÃ³n Sesgada en un Departamento de InformÃ¡tica

### ğŸ” Contexto
Un sistema de IA para contrataciÃ³n en el sector informÃ¡tico muestra claros sesgos de gÃ©nero en la selecciÃ³n de candidatos, reflejando y amplificando los prejuicios histÃ³ricos en la industria tecnolÃ³gica.

## ğŸ§© Actividad Principal: AnÃ¡lisis del Caso en Grupos

### ğŸ“‹ Instrucciones de Trabajo
Los participantes se dividirÃ¡n en grupos para abordar tres aspectos fundamentales del sesgo en IA:

1. **ğŸ•µï¸â€â™€ï¸ IdentificaciÃ³n de Fuentes de Sesgo**
   - Examinar los datos de entrenamiento
   - Detectar patrones de discriminaciÃ³n
   - Analizar la representatividad de los datos

2. **ğŸ”¬ AnÃ¡lisis de AmplificaciÃ³n de Sesgos**
   - Comprender cÃ³mo el modelo reproduce y potencia los sesgos existentes
   - Identificar mecanismos de perpetuaciÃ³n de desigualdades
   - Evaluar el impacto en los procesos de selecciÃ³n

3. **ğŸ› ï¸ Propuestas de MitigaciÃ³n**
   - Desarrollar estrategias para reducir sesgos
   - Proponer mejoras en la recopilaciÃ³n y tratamiento de datos
   - DiseÃ±ar mÃ©todos de correcciÃ³n algorÃ­tmica

## ğŸ“… Agenda del Taller

### 1. ğŸ§  Mapa Mental de DesafÃ­os Ã‰ticos (Fase de ClarificaciÃ³n) - 10 minutos
- Unirse a grupos de trabajo
- Identificar desafÃ­os Ã©ticos en IA:
  * ğŸ” Transparencia
  * ğŸŒˆ Diversidad
  * âš–ï¸ Responsabilidad
  * ğŸ”’ Privacidad

### 2. ğŸ’¡ Brainwriting 6-3-5 (Fase de IdeaciÃ³n) - 5 minutos
- **6 personas** > **3 ideas** > en **5 minutos**

>  Â¿CÃ³mo abordar desafÃ­os Ã©ticos en la construcciÃ³n y ajuste de modelos de machine learning  - inteligencia artificial?


### 3. ğŸš€ Prototipado RÃ¡pido (Fase de Desarrollo) - 10 minutos
- Desarrollar un prototipo de IA Ã©tica
- Consideraciones clave:
  * ğŸ“Š Diversidad de datos
  * ğŸ” Transparencia
  * ğŸŒ Impacto social

### 4. ğŸ—£ï¸ PresentaciÃ³n de Prototipos (Fase de ImplementaciÃ³n) - 10 minutos
- Cada grupo presenta su prototipo en **1-2 minutos**

## ğŸ§  Consideraciones Finales
- Reflexionar sobre la responsabilidad Ã©tica en el desarrollo de IA
- Comprender la importancia de la equidad y la inclusiÃ³n
- Desarrollar estrategias prÃ¡cticas para mitigar sesgos

*Nota: Este taller busca fomentar una reflexiÃ³n crÃ­tica sobre los sesgos en sistemas de inteligencia artificial, promoviendo un desarrollo tecnolÃ³gico mÃ¡s justo e inclusivo.* ğŸŒŸğŸ¤

# ğŸ“š Material complementario: SimulaciÃ³n de AnÃ¡lisis de Factores para la ContrataciÃ³n de Personal

Este repositorio contiene un proyecto de anÃ¡lisis de datos con el objetivo de explorar los factores que influyen en la **contrataciÃ³n** de personal dentro de una empresa de tecnologÃ­a. ğŸ–¥ï¸ Utilizando tÃ©cnicas de anÃ¡lisis estadÃ­stico y de machine learning, este proyecto busca identificar patrones y correlaciones entre diversas caracterÃ­sticas de los candidatos (edad, gÃ©nero, aÃ±os de experiencia, habilidades, salario previo, entre otros) y la variable de **ContrataciÃ³n**. ğŸ”

## ğŸ¯ Objetivos del Caso
- **Explorar** y **preprocesar** datos relacionados con la contrataciÃ³n de personal. ğŸ•µï¸â€â™€ï¸
- **Analizar** la relaciÃ³n entre diversas variables (como gÃ©nero, edad, aÃ±os de experiencia) y la variable objetivo **Contratacion**. ğŸ“Š
- **Construir modelos de machine learning** para predecir la contrataciÃ³n de un candidato, evaluando el impacto de las variables sensibles (como gÃ©nero) y aplicando estrategias de fairness (justicia algorÃ­tmica). ğŸ¤–
- **Visualizar** las distribuciones y correlaciones de las variables mediante herramientas como *pairplots*, matrices de correlaciÃ³n, etc. ğŸ“ˆ

## ğŸ”¬ DescripciÃ³n del Proyecto
### 1. ğŸ” **AnÃ¡lisis Exploratorio de Datos (EDA)**
   - Limpieza y transformaciÃ³n de los datos. ğŸ§¹
   - AnÃ¡lisis descriptivo de las variables, incluyendo estadÃ­sticas clave como medias, desviaciones estÃ¡ndar, y correlaciones entre las caracterÃ­sticas. ğŸ“‰
   - VisualizaciÃ³n de las distribuciones y relaciones entre las variables usando grÃ¡ficos de dispersiÃ³n, histogramas y mapas de calor. ğŸŒˆ

### 2. ğŸ› ï¸ **Preprocesamiento de Datos**
   - ConversiÃ³n de variables categÃ³ricas a variables numÃ©ricas utilizando tÃ©cnicas como `one-hot encoding`. ğŸ”¢
   - Tratamiento y normalizaciÃ³n de las atributos. âœ¨

### 3. ğŸ¤– **Modelos de Machine Learning**
   - CreaciÃ³n de modelos predictivos como **Logistic Regression**, evaluando el rendimiento de cada modelo mediante mÃ©tricas de error. ğŸ§®
   - ImplementaciÃ³n de penalizaciones de **fairness** (justicia algorÃ­tmica) en los modelos para asegurarse de que no haya sesgos hacia ciertos grupos (por ejemplo, gÃ©nero). âš–ï¸
   - ComparaciÃ³n de modelos que excluyen o ajustan variables sensibles para evaluar el impacto en la equidad y precisiÃ³n de las predicciones. ğŸ¤

### 4. ğŸ”¢ **MÃ©tricas de Fairness**
   - CÃ¡lculo de disparidades en precisiÃ³n, tasa de falsos positivos y tasa de falsos negativos entre diferentes grupos (por ejemplo, hombres, mujeres y personas no binarias). ğŸ“Š
   - EvaluaciÃ³n de cÃ³mo las diferentes estrategias de penalizaciÃ³n afectan el rendimiento y la equidad de los modelos. ğŸŒŸ

## ğŸ“Œ Requisitos
- Python 3.7 o superior ğŸ
- LibrerÃ­as necesarias:
  - `pandas` ğŸ“Š
  - `numpy` ğŸ“
  - `scikit-learn` ğŸ¤–
  - `seaborn` ğŸŒˆ
  - `matplotlib` ğŸ“ˆ
  - `scipy` ğŸ§®

## ğŸš€ InstalaciÃ³n

1. Clona este repositorio:

```bash
   git clone https://github.com/tu_usuario/analisis-contratacion.git
```

# ğŸŒ MÃ©tricas Clave en Fairness

Las mÃ©tricas FPR y TPR son fundamentales para la evaluaciÃ³n de fairness en modelos de aprendizaje automÃ¡tico porque permiten identificar y corregir posibles sesgos en el rendimiento del modelo hacia diferentes grupos. Estas mÃ©tricas ayudan a asegurarse de que el modelo no estÃ© favoreciendo ni perjudicando injustamente a ningÃºn grupo, lo cual es esencial para el uso Ã©tico y justo de la inteligencia artificial.

## Fairness en ML
El objetivo es desarrollar modelos de aprendizaje automÃ¡tico que sean justos y no discriminen a ciertos grupos de personas. Esto es particularmente importante cuando los modelos se usan en Ã¡reas como contrataciÃ³n, justicia penal, salud, etc., donde las decisiones basadas en estos modelos pueden tener grandes implicaciones.

## FPR y TPR en Fairness:
- TPR (True Positive Rate), tambiÃ©n conocido como sensibilidad o recall, mide la capacidad del modelo para identificar correctamente a los positivos reales de cada grupo.
- FPR (False Positive Rate) mide la proporciÃ³n de elementos negativos que fueron incorrectamente clasificados como positivos. En tÃ©rminos de equidad, una alta tasa de falsos positivos podrÃ­a ser problemÃ¡tica si un grupo estÃ¡ siendo penalizado errÃ³neamente.
  
## Fairness y Equidad en el Rendimiento
Las mÃ©tricas de FPR y TPR se utilizan en fairness porque permiten medir disparidades en el rendimiento del modelo entre diferentes subgrupos. En otras palabras, estas mÃ©tricas se enfocan en si el modelo estÃ¡ tratando de manera equitativa a los diferentes grupos. Ejemplos de mÃ©tricas de fairness basadas en FPR y TPR

1. Demographic Parity:
   - Busca que un modelo tome decisiones de manera equitativa entre grupos, sin que algÃºn grupo tenga mÃ¡s probabilidades de ser seleccionado que otro.
   - Para esto, podrÃ­amos comparar las tasas de TPR o FPR entre grupos. Si un grupo tiene una tasa significativamente mÃ¡s alta de TPR que otro, se podrÃ­a concluir que el modelo estÃ¡ favoreciendo a ese grupo en tÃ©rminos de identificaciÃ³n de positivos.
2. Equal Opportunity:
   - Una variante mÃ¡s fuerte de Demographic Parity se basa en TPR: los modelos deben ser igualmente buenos para identificar positivos entre los diferentes grupos. Si un grupo tiene una TPR significativamente mÃ¡s baja, el modelo no estÃ¡ siendo justo.
   - En este caso, lo ideal serÃ­a que la tasa de verdaderos positivos (TPR) para cada grupo fuera similar (es decir, el modelo deberÃ­a ser igual de efectivo para detectar verdaderos positivos en todos los grupos).

3. Equalized Odds:
   - Esta es una condiciÃ³n de fairness que asegura que tanto TPR como FPR sean iguales entre los diferentes grupos.
   - Esto implica que el modelo debe tener tanto una tasa de verdaderos positivos como una tasa de falsos positivos iguales entre grupos. Si un grupo tiene una FPR mÃ¡s alta o una TPR mÃ¡s baja, esto indica que el modelo es sesgado en contra de ese grupo.
   - Si FPR y TPR son desbalanceados entre grupos, esto sugiere que el modelo podrÃ­a estar favoreciendo o perjudicando injustamente a un grupo en particular.

4. Fairness Through Unawareness:
   - A veces se usa una versiÃ³n de fairness en la que se asegura que el modelo no utiliza caracterÃ­sticas sensibles (como el gÃ©nero o la raza) directamente. Sin embargo, aÃºn pueden surgir disparidades en FPR y TPR entre diferentes grupos, incluso si esas caracterÃ­sticas no se usan directamente.

### Â¿Por quÃ© FPR y TPR son clave en fairness?

- Equidad en las decisiones: Las mÃ©tricas de FPR y TPR son esenciales porque miden el rendimiento diferencial del modelo entre diferentes grupos. Si el modelo tiene un alto FPR para un grupo pero no para otro, podrÃ­a indicar que el modelo estÃ¡ discriminando injustamente contra ese grupo, generando mÃ¡s falsos positivos para ese grupo.
- Impacto en grupos desproporcionados: Por ejemplo, en un sistema de justicia penal automatizado, un modelo que tenga una alta FPR (falsos positivos) para una minorÃ­a podrÃ­a generar mÃ¡s encarcelamientos errÃ³neos para ese grupo, lo que serÃ­a un perjuicio para su bienestar. Del mismo modo, si un grupo tiene una baja TPR (es decir, el modelo no identifica correctamente a los individuos positivos de ese grupo), esto podrÃ­a significar que estÃ¡n siendo injustamente desatendidos por el modelo.

### AplicaciÃ³n de FPR y TPR en fairness

Al evaluar un modelo para fairness, los FPR y TPR pueden utilizarse de la siguiente manera:
-  Si un modelo tiene una TPR significativamente mÃ¡s baja para un grupo (por ejemplo, mujeres o personas no binarias), esto indica que el modelo no estÃ¡ reconociendo adecuadamente a los miembros de ese grupo cuando deberÃ­an ser clasificados como positivos (por ejemplo, identificando correctamente a personas que califican para un beneficio o programa).
-  Si un modelo tiene una FPR significativamente mÃ¡s alta para un grupo, significa que hay mÃ¡s casos de falsos positivos para ese grupo, lo que podrÃ­a resultar en penalizaciones incorrectas o tratamientos adversos.


<br>

---- 
*Â¡Un proyecto dedicado al uso y generaciÃ³n de modelos de AI-ML mÃ¡s justos y equitativos ğŸŒğŸ¤*
---
